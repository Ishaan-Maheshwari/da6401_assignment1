# **Feedforward Neural Network Training & Hyperparameter Sweeping**  

This repository provides a **custom feedforward neural network** built from scratch and allows:  
- Training the model using `train.py`  
- Running **automated hyperparameter tuning** using `sweep.py`  
- Logging results to **Weights & Biases (wandb)**  

---

## **ğŸ“Œ Index**
1. [Installation & Setup](#installation--setup)  
2. [Project Structure](#project-structure)  
3. [How to Train the Model (`train.py`)](#how-to-train-the-model-trainpy)  
4. [How to Run a Hyperparameter Sweep (`sweep.py`)](#how-to-run-a-hyperparameter-sweep-sweeppy)  
5. [GitHub Repository](#github-repository)
6. [WandB Report](#wandb-report)


## **1ï¸âƒ£ Installation & Setup**  

### **ğŸ”¹ Install Required Dependencies**  
Make sure you have Python **3.7+** installed. Then, install required packages:  

```bash
pip install -r requirements.txt
```

### **ğŸ”¹ Set Up Weights & Biases (wandb)**  
Before running the training or sweep, log in to **Weights & Biases**:  
```bash
wandb login
```
If you donâ€™t have a WandB account, create one at [https://wandb.ai](https://wandb.ai).

---

# **2ï¸âƒ£ Project Structure**  

The project follows a **modular structure**, making it easy to maintain, expand, and reuse different components. Each module is responsible for a specific task, such as dataset handling, model creation, optimizers, training, and hyperparameter tuning.

---

## **ğŸ“‚ Root Directory Overview**
```
/root_directory
â”‚â”€â”€ train.py        # Main script to train the neural network
â”‚â”€â”€ sweep.py        # Script to run a WandB hyperparameter sweep
â”‚â”€â”€ my_nn/          # Neural network module (contains all core implementations)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dataset.py
â”‚   â”œâ”€â”€ activations.py
â”‚   â”œâ”€â”€ initializers.py
â”‚   â”œâ”€â”€ optimizers.py
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ trainer.py
â”‚â”€â”€ README.md       # Documentation on how to use the project
```
---

## **ğŸ“‚ Detailed Breakdown of Each Component**

### **1ï¸âƒ£ `train.py` â€“ Training Script**
ğŸ“Œ **Purpose:**  
- Allows users to train the neural network with **custom hyperparameters**.
- Accepts command-line arguments to modify model settings.
- Logs training results to **Weights & Biases (wandb)**.

ğŸ“Œ **Key Features:**  
âœ” Parses command-line arguments (epochs, batch size, optimizer, etc.).  
âœ” Loads dataset and initializes the neural network.  
âœ” Runs training loop and logs performance.  
âœ” Handles errors (invalid hyperparameters, WandB connection issues, etc.).  

---

### **2ï¸âƒ£ `sweep.py` â€“ Hyperparameter Tuning Script**
ğŸ“Œ **Purpose:**  
- Runs a **WandB sweep**, automatically training the model with different hyperparameters.
- Helps find the best settings for **optimal accuracy and loss**.

ğŸ“Œ **Key Features:**  
âœ” Defines a sweep **configuration** (ranges for hyperparameters).  
âœ” Runs multiple **experiments** with different hyperparameter combinations.  
âœ” Logs results to **WandB**, making it easy to analyze best-performing settings.  

---

### **3ï¸âƒ£ `my_nn/` â€“ Neural Network Package**
This directory contains the core implementation of the **feedforward neural network**.

---

#### **ğŸ“‚ `my_nn/__init__.py` â€“ Package Initialization**
ğŸ“Œ **Purpose:**  
- Initializes the **my_nn** package, making it easy to import different components.  
- Defines which modules can be accessed directly.  


---

#### **ğŸ“‚ `my_nn/dataset.py` â€“ Dataset Handling**
ğŸ“Œ **Purpose:**  
- Loads **MNIST** or **Fashion-MNIST** datasets.  
- Preprocesses data (normalization, reshaping, one-hot encoding).  
- Splits dataset into **train, validation, and test sets**.  


---

#### **ğŸ“‚ `my_nn/activations.py` â€“ Activation Functions**
ğŸ“Œ **Purpose:**  
- Implements **activation functions** and their derivatives for backpropagation.  

ğŸ“Œ **Supported Activations:**  
âœ” Sigmoid  
âœ” Tanh  
âœ” ReLU  
âœ” Identity (for linear layers)  



---

#### **ğŸ“‚ `my_nn/initializers.py` â€“ Weight Initialization**
ğŸ“Œ **Purpose:**  
- Defines different **weight initialization methods** to improve model performance.  

ğŸ“Œ **Supported Methods:**  
âœ” **Random Initialization** (Small Gaussian noise)  
âœ” **Xavier Initialization** (For deep networks, prevents exploding/vanishing gradients)  


---

#### **ğŸ“‚ `my_nn/optimizers.py` â€“ Optimization Algorithms**
ğŸ“Œ **Purpose:**  
- Implements different **optimizers** for training the model.  

ğŸ“Œ **Supported Optimizers:**  
âœ” **SGD (Stochastic Gradient Descent)**  
âœ” **Momentum (Improves SGD convergence)**  
âœ” **NAG (Nesterov Accelerated Gradient)**  
âœ” **RMSprop (Adaptive learning rate optimizer)**  
âœ” **Adam (Most commonly used optimizer)**  
âœ” **Nadam (Adam + Nesterov Momentum)**  


---

#### **ğŸ“‚ `my_nn/model.py` â€“ Neural Network Model**
ğŸ“Œ **Purpose:**  
- Implements **forward and backward propagation**.  
- Supports **custom architectures** with multiple hidden layers.  
- Uses **different activation functions & optimizers**.  

ğŸ“Œ **Key Methods:**  
âœ” `forward(X)`: Compute activations layer-by-layer.  
âœ” `backward(X, y_true, activations)`: Compute gradients for backpropagation.  
âœ” `update_weights(grads)`: Apply optimizer updates.  


---

#### **ğŸ“‚ `my_nn/trainer.py` â€“ Training Logic**
ğŸ“Œ **Purpose:**  
- Runs **training loop** and updates weights using selected optimizer.  
- Logs training results to **WandB**.  

ğŸ“Œ **Key Features:**  
âœ” Runs multiple **epochs**, updates weights in **mini-batches**.  
âœ” Computes **loss & accuracy** on training and validation sets.  
âœ” Logs everything in **Weights & Biases (wandb)**.  

---

## **3ï¸âƒ£ How to Train the Model (`train.py`)**  

The `train.py` script **trains the neural network** using custom hyperparameters and logs results to **WandB**.

### **ğŸ”¹ Basic Usage**
```bash
python train.py --wandb_entity myname --wandb_project myprojectname
```
This will train the model using **default hyperparameters**.

### **ğŸ”¹ Customizing Hyperparameters**
You can change hyperparameters by passing them as command-line arguments:
```bash
python train.py --epochs 10 --batch_size 16 --optimizer adam --learning_rate 0.001
```

### **ğŸ”¹ Available Arguments**
| Argument | Default | Description |
|----------|---------|-------------|
| `-wp`, `--wandb_project` | `myprojectname` | Name of the WandB project |
| `-we`, `--wandb_entity` | `myname` | WandB entity name |
| `-d`, `--dataset` | `fashion_mnist` | Dataset (`mnist` or `fashion_mnist`) |
| `-e`, `--epochs` | `1` | Number of training epochs |
| `-b`, `--batch_size` | `4` | Batch size |
| `-l`, `--loss` | `cross_entropy` | Loss function (`mean_squared_error` or `cross_entropy`) |
| `-o`, `--optimizer` | `sgd` | Optimizer (`sgd`, `adam`, `nadam`, etc.) |
| `-lr`, `--learning_rate` | `0.1` | Learning rate |
| `-m`, `--momentum` | `0.5` | Momentum for `momentum` and `nag` optimizers |
| `-beta`, `--beta` | `0.5` | Beta for `rmsprop` |
| `-beta1`, `--beta1` | `0.5` | Beta1 for `adam` and `nadam` |
| `-beta2`, `--beta2` | `0.5` | Beta2 for `adam` and `nadam` |
| `-eps`, `--epsilon` | `1e-6` | Epsilon for numerical stability |
| `-w_d`, `--weight_decay` | `0.0` | Weight decay |
| `-w_i`, `--weight_init` | `random` | Weight initialization (`random` or `Xavier`) |
| `-nhl`, `--num_layers` | `1` | Number of hidden layers |
| `-sz`, `--hidden_size` | `4` | Neurons per hidden layer |
| `-a`, `--activation` | `sigmoid` | Activation function (`identity`, `sigmoid`, `tanh`, `ReLU`) |

---

## **4ï¸âƒ£ How to Run a Hyperparameter Sweep (`sweep.py`)**  

The `sweep.py` script **automatically searches for the best hyperparameters** using **WandB sweeps**.

### **ğŸ”¹ Basic Usage**
```bash
python sweep.py --wandb_entity myname --wandb_project myprojectname --count 10
```
This runs **10 different training experiments** with random hyperparameter values.

### **ğŸ”¹ Available Arguments**
| Argument | Default | Description |
|----------|---------|-------------|
| `-wp`, `--wandb_project` | `myprojectname` | Name of the WandB project |
| `-we`, `--wandb_entity` | `myname` | WandB entity name |
| `-c`, `--count` | `10` | Number of sweep runs |

### **ğŸ”¹ What Happens in a Sweep?**
1. `sweep.py` initializes a **WandB sweep** with predefined hyperparameter ranges.
2. It runs multiple **training experiments**, each with a different set of hyperparameters.
3. Logs results to WandB, helping you find the **best-performing configuration**.

---

## **5ï¸âƒ£ GitHub Repository**

You can find the GitHub repository for this project [here](https://github.com/Ishaan-Maheshwari/da6401_assignment1.git).

---

## **6ï¸âƒ£ WandB Report**
The link to the W&B report is: [https://wandb.ai/ishaan_maheshwari-indian-institute-of-technology-madras/assignment-1/reports/Ishaan-Maheshwari-s-DA6401-Assignment-1--VmlldzoxMTgyMzQwNQ?accessToken=qmqrmp6mxgsqt3tzvrm5yh7fubh94l108hn6qqtvxo5epipaekwlxa6lzvbk282e](https://wandb.ai/ishaan_maheshwari-indian-institute-of-technology-madras/assignment-1/reports/Ishaan-Maheshwari-s-DA6401-Assignment-1--VmlldzoxMTgyMzQwNQ?accessToken=qmqrmp6mxgsqt3tzvrm5yh7fubh94l108hn6qqtvxo5epipaekwlxa6lzvbk282e)

---
